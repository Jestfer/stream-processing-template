{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 09:07:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/home/ec2-user/pyspark_streaming/lib64/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "# Define the path to the jars on the EC2 instance\n",
    "spark_jars_path = \"/home/ec2-user/stream-processing-template/jars\"  # <-- Update this path\n",
    "\n",
    "spark = SparkSession.builder.appName(\"retail_pysaprk_consumer\") \\\n",
    "    .config(\"spark.jars\", f\"{spark_jars_path}/commons-pool2-2.11.1.jar,\"\n",
    "            f\"{spark_jars_path}/spark-sql-kafka-0-10_2.12-3.4.0.jar,\"\n",
    "            f\"{spark_jars_path}/spark-streaming-kafka-0-10-assembly_2.12-3.4.0.jar\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for our data\n",
    "schema = StructType([\n",
    "    StructField(\"store_location\", StringType(), True),\n",
    "    StructField(\"time_of_purchase\", TimestampType(), True),\n",
    "    StructField(\"product_ID\", StringType(), True),\n",
    "    StructField(\"transaction_amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Stream from Kafka topic\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"b-1.monstercluster1.6xql65.c3.kafka.eu-west-2.amazonaws.com:9092\") \\\n",
    "    .option(\"subscribe\", \"retail_transactions\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = (df.selectExpr(\"CAST(value AS STRING)\")\n",
    "                .withColumn(\"data\", from_json(col(\"value\"), schema))\n",
    "                .select(\"data.*\"))\n",
    "\n",
    "query = transactions.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your use case, you only need to start the Kafka stream once. The `transactions` DataFrame you defined will continuously read the data from Kafka. Each of your transformation operations (like aggregations, filtering, etc.) will then be applied to this `transactions` DataFrame.\n",
    "\n",
    "However, when you want to view the results of these transformations, you have to initiate separate streaming queries for each operation. The key is to define these transformations as separate queries from the initial stream read.\n",
    "\n",
    "Hereâ€™s how you can structure this:\n",
    "\n",
    "1. **Start the Kafka Stream**: Run this block once to initialize the streaming DataFrame from Kafka.\n",
    "\n",
    "   ```python\n",
    "   transactions = (df.selectExpr(\"CAST(value AS STRING)\")\n",
    "                   .withColumn(\"data\", from_json(col(\"value\"), schema))\n",
    "                   .select(\"data.*\"))\n",
    "   ```\n",
    "\n",
    "2. **Apply Transformations and View Results**: For each transformation, you'll create a new streaming query. Here's how you would modify the provided examples to view their output:\n",
    "\n",
    "   **Example - Total Transaction Amounts by Store Location**:\n",
    "\n",
    "   ```python\n",
    "   total_amounts_by_location = transactions.groupBy(\"store_location\")\\\n",
    "                                           .agg(_sum(\"transaction_amount\").alias(\"total_amount\"))\n",
    "\n",
    "   query_total_amounts = total_amounts_by_location.writeStream \\\n",
    "       .outputMode(\"complete\") \\\n",
    "       .format(\"console\") \\\n",
    "       .start()\n",
    "\n",
    "   query_total_amounts.awaitTermination()\n",
    "   ```\n",
    "\n",
    "   You would replicate this pattern for each of your other transformations, creating separate queries and viewing their results independently.\n",
    "\n",
    "3. **Managing Multiple Streams**: If you start multiple queries like this, each will output to the console independently. To view only the output of a specific transformation, you should stop other active queries using `query.stop()` before starting a new one.\n",
    "\n",
    "4. **Note on `.awaitTermination()`**: Be aware that `query.awaitTermination()` will block the current thread until the stream query stops, which means you won't be able to run other code in the same notebook or script while the query is active. If you're experimenting in a Jupyter Notebook, you might want to remove the `.awaitTermination()` call and rely on the notebook's interactive environment to manage the stream's lifecycle. Alternatively, for long-running streams or automated scripts, `.awaitTermination()` is appropriate.\n",
    "\n",
    "By structuring your Jupyter Notebook this way, you can effectively manage multiple transformations on your Kafka stream and view each transformation's output independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transformations:\n",
    "Aggregations:\n",
    "\n",
    "Total transaction amounts by store location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "total_amounts_by_location = transactions.groupBy(\"store_location\")\\\n",
    "                                        .agg(_sum(\"transaction_amount\").alias(\"total_amount\"))\n",
    "\n",
    "query_total_amounts = total_amounts_by_location.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_total_amounts.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After viewing results from above snippet, and when we are ready to stop the stream...\n",
    "\n",
    "query_total_amounts.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.2: Number of transactions by store location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transactions_by_location = transactions.groupBy(\"store_location\")\\\n",
    "                                           .count()\\\n",
    "                                           .withColumnRenamed(\"count\", \"num_transactions\")\n",
    "\n",
    "query_num_transactions = num_transactions_by_location.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_num_transactions.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_num_transactions.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.3: Average Transaction Amount by Store Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_transaction_amount_by_location = transactions.groupBy(\"store_location\")\\\n",
    "                                                 .agg(avg(\"transaction_amount\").alias(\"avg_transaction_amount\"))\n",
    "\n",
    "query_avg_transaction_amount = avg_transaction_amount_by_location.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_avg_transaction_amount.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_avg_transaction_amount.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Time Series Analysis\n",
    "2.1 Transactions per Hour/Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, dayofmonth\n",
    "\n",
    "transactions_per_hour = transactions.withColumn(\"hour\", hour(\"time_of_purchase\"))\\\n",
    "                                    .groupBy(\"hour\")\\\n",
    "                                    .count()\\\n",
    "                                    .withColumnRenamed(\"count\", \"transactions_per_hour\")\n",
    "\n",
    "query_transactions_per_hour = transactions_per_hour.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_transactions_per_hour.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_transactions_per_hour.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_per_day = transactions.withColumn(\"day\", dayofmonth(\"time_of_purchase\"))\\\n",
    "                                   .groupBy(\"day\")\\\n",
    "                                   .count()\\\n",
    "                                   .withColumnRenamed(\"count\", \"transactions_per_day\")\n",
    "\n",
    "query_transactions_per_day = transactions_per_day.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_transactions_per_day.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_transactions_per_day.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Total or Average Transaction Amounts Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "total_amounts_over_time = transactions.groupBy(dayofmonth(\"time_of_purchase\").alias(\"day\"))\\\n",
    "                                      .agg(_sum(\"transaction_amount\").alias(\"total_amount\"))\n",
    "\n",
    "query_total_amounts_over_time = total_amounts_over_time.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_total_amounts_over_time.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_total_amounts_over_time.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_amounts_over_time = transactions.groupBy(dayofmonth(\"time_of_purchase\").alias(\"day\"))\\\n",
    "                                    .agg(avg(\"transaction_amount\").alias(\"avg_amount\"))\n",
    "\n",
    "query_avg_amounts_over_time = avg_amounts_over_time.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_avg_amounts_over_time.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_avg_amounts_over_time.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Product Analysis\n",
    "Top N Products by Number of Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_products_by_transactions = transactions.groupBy(\"product_ID\")\\\n",
    "                                             .count()\\\n",
    "                                             .withColumnRenamed(\"count\", \"num_transactions\")\\\n",
    "                                             .orderBy(col(\"num_transactions\").desc())\n",
    "\n",
    "query_top_n_products_transactions = top_n_products_by_transactions.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_top_n_products_transactions.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_top_n_products_transactions.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Top N Products by Total Transaction Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_products_by_total_amount = transactions.groupBy(\"product_ID\")\\\n",
    "                                             .agg(_sum(\"transaction_amount\").alias(\"total_transaction_amount\"))\\\n",
    "                                             .orderBy(col(\"total_transaction_amount\").desc())\n",
    "\n",
    "query_top_n_products_total_amount = top_n_products_by_total_amount.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_top_n_products_total_amount.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_top_n_products_total_amount.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Anomaly Detection\n",
    "Identify Transactions with Amounts Significantly Different from the Average\n",
    "\n",
    "--\n",
    "\n",
    "The error you're encountering, \"AnalysisException: Queries with streaming sources must be executed with writeStream.start(); kafka\", indicates that there is an issue with how you're attempting to process the streaming data from Kafka. This error typically occurs when you're trying to use an action (like `.show()`, `.collect()`, etc.) that is meant for batch processing on a streaming DataFrame.\n",
    "\n",
    "In your case, the issue seems to be arising from these lines:\n",
    "\n",
    "```python\n",
    "std_dev = transactions.agg(stddev(\"transaction_amount\").alias(\"stddev_amount\")).collect()[0][\"stddev_amount\"]\n",
    "avg_amount = transactions.agg(avg(\"transaction_amount\").alias(\"avg_amount\")).collect()[0][\"avg_amount\"]\n",
    "```\n",
    "\n",
    "In a streaming context, you cannot directly use actions such as `collect()` on a streaming DataFrame, since it represents an unbounded dataset. The standard aggregation functions and actions like `collect()` are designed for batch DataFrames where the dataset is finite and fully available at the time of computation.\n",
    "\n",
    "To work around this, you need to define your anomaly detection logic within the streaming query itself, and then start the query with `writeStream`. However, calculating a standard deviation or average in real-time over a streaming dataset can be complex and may require using stateful operations or windowed aggregations.\n",
    "\n",
    "Here's an alternative approach using windowed aggregations for streaming data:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window, stddev, avg\n",
    "\n",
    "# Windowed aggregation for streaming data\n",
    "windowed_transactions = transactions \\\n",
    "    .withColumn(\"window\", window(\"time_of_purchase\", \"1 hour\")) \\\n",
    "    .groupBy(\"window\") \\\n",
    "    .agg(\n",
    "        stddev(\"transaction_amount\").alias(\"stddev_amount\"),\n",
    "        avg(\"transaction_amount\").alias(\"avg_amount\")\n",
    "    )\n",
    "\n",
    "# You can then start the stream to write the results to the console\n",
    "query_windowed_transactions = windowed_transactions.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_windowed_transactions.awaitTermination()\n",
    "```\n",
    "\n",
    "This code snippet uses a tumbling window of 1 hour to calculate the standard deviation and average of transaction amounts. Each window will be processed independently. Remember that in a streaming context, especially with windowed aggregations, your results are calculated over the data in the respective windows and not over the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, stddev, avg\n",
    "\n",
    "# Windowed aggregation for streaming data\n",
    "windowed_transactions = transactions \\\n",
    "    .withColumn(\"window\", window(\"time_of_purchase\", \"1 hour\")) \\\n",
    "    .groupBy(\"window\") \\\n",
    "    .agg(\n",
    "        stddev(\"transaction_amount\").alias(\"stddev_amount\"),\n",
    "        avg(\"transaction_amount\").alias(\"avg_amount\")\n",
    "    )\n",
    "\n",
    "# You can then start the stream to write the results to the console\n",
    "query_anomalous_transactions = windowed_transactions.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_anomalous_transactions.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 26\n",
      "-------------------------------------------\n",
      "+--------------------+------------------+-----------------+\n",
      "|              window|     stddev_amount|       avg_amount|\n",
      "+--------------------+------------------+-----------------+\n",
      "|{2023-12-12 09:00...|359.34876686639353|452.1512605042017|\n",
      "+--------------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 179:================================>                    (123 + 1) / 200]\r"
     ]
    }
   ],
   "source": [
    "query_anomalous_transactions.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Filtering\n",
    "Filter Transactions Below or Above a Certain Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_transactions = transactions.filter((col(\"transaction_amount\") > 50) & \n",
    "                                            (col(\"transaction_amount\") < 1000))\n",
    "\n",
    "query_filtered_transactions = filtered_transactions.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_filtered_transactions.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_filtered_transactions.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_streaming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
