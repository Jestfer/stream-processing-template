{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/12 08:34:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/home/ec2-user/pyspark_streaming/lib64/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|first_name|last_name|age|\n",
      "+----------+---------+---+\n",
      "|      John|      Doe| 29|\n",
      "|      Jane|    Smith| 34|\n",
      "|       Sam|    Brown| 23|\n",
      "+----------+---------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"IntroToPySpark\").getOrCreate()\n",
    "\n",
    "data = [(\"John\", \"Doe\", 29), (\"Jane\", \"Smith\", 34), (\"Sam\", \"Brown\", 23)]\n",
    "columns = [\"first_name\", \"last_name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|first_name|age|\n",
      "+----------+---+\n",
      "|      John| 29|\n",
      "|      Jane| 34|\n",
      "|       Sam| 23|\n",
      "+----------+---+\n",
      "\n",
      "+----------+---------+---+\n",
      "|first_name|last_name|age|\n",
      "+----------+---------+---+\n",
      "|      John|      Doe| 29|\n",
      "|      Jane|    Smith| 34|\n",
      "+----------+---------+---+\n",
      "\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 29|    1|\n",
      "| 34|    1|\n",
      "| 23|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting specific columns\n",
    "df.select(\"first_name\", \"age\").show()\n",
    "\n",
    "# Filtering data\n",
    "df.filter(df[\"age\"] > 25).show()\n",
    "\n",
    "# Grouping and Aggregating\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|first_name|last_name|age|\n",
      "+----------+---------+---+\n",
      "|      John|      Doe| 29|\n",
      "|      Jane|    Smith| 34|\n",
      "+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 25\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, let's break down what this code is doing in PySpark and explain the concept of an RDD.\n",
    "\n",
    "1. **Understanding the Code**:\n",
    "   - `rdd = spark.sparkContext.parallelize([1,2,3,4,5])`: This line creates an RDD (Resilient Distributed Dataset) named `rdd`. It does this by parallelizing a list `[1,2,3,4,5]`, which means distributing the list elements across multiple nodes in the cluster. Each element (1, 2, 3, 4, 5) becomes a separate item in the RDD.\n",
    "   - `squared_rdd = rdd.map(lambda x: x*x)`: This line applies a transformation to each element of the `rdd`. The `map` function takes a lambda function that squares each element. So, `squared_rdd` becomes an RDD containing the squares of the original numbers (1, 4, 9, 16, 25).\n",
    "   - `print(squared_rdd.collect())`: Finally, `collect()` is an action that retrieves all elements of the `squared_rdd` from the distributed cluster and brings them back to the local machine as a regular Python list. The `print` statement then outputs this list.\n",
    "\n",
    "2. **What is an RDD in PySpark?**:\n",
    "   - RDD stands for Resilient Distributed Dataset. It's a fundamental data structure of PySpark that represents an immutable, distributed collection of objects. Each dataset in an RDD is divided into logical partitions, which may be computed on different nodes of the cluster.\n",
    "   - **Resilient**: RDDs are fault-tolerant, meaning they can automatically recover from node failures.\n",
    "   - **Distributed**: Data in RDDs is distributed across multiple nodes in a cluster, allowing for parallel processing.\n",
    "   - **Dataset**: Refers to a collection of partitioned data with values.\n",
    "\n",
    "RDDs are the backbone of PySpark, enabling it to handle big data processing efficiently by utilizing distributed computing. They allow users to perform transformations (like `map`, `filter`) and actions (like `collect`, `count`) on large datasets in a distributed environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "squared_rdd = rdd.map(lambda x: x*x)\n",
    "print(squared_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This PySpark code demonstrates the process of performing linear regression, a fundamental machine learning task, using the PySpark ML (Machine Learning) library. Here's a high-level overview of what each part of the code is doing:\n",
    "\n",
    "1. **Import Libraries**: \n",
    "   - `LinearRegression`: The linear regression model from PySpark's machine learning library.\n",
    "   - `Vectors`: A utility for handling local vectors that are not distributed across multiple machines.\n",
    "   - `VectorAssembler`: A transformer that combines multiple columns into a single vector column, often used to prepare data for machine learning models.\n",
    "\n",
    "2. **Prepare Sample Data**:\n",
    "   - The `data` list contains tuples, each representing a data point. Each tuple has two elements: a feature vector (using `Vectors.dense`) and a label.\n",
    "   - `Vectors.dense([0.0])`, `Vectors.dense([1.0])`, and `Vectors.dense([2.0])` are feature vectors. In this simple case, each vector contains only one feature.\n",
    "   - The corresponding labels are `1.0`, `2.0`, and `3.0`.\n",
    "\n",
    "3. **Create DataFrame**:\n",
    "   - The data is converted into a DataFrame `df`, with columns named \"features\" and \"label\". This is a standard format for ML tasks in PySpark, where features are usually presented in vector form.\n",
    "\n",
    "4. **Set Up Linear Regression Model**:\n",
    "   - An instance of `LinearRegression` is created with specific parameters (`maxIter`, `regParam`, and `elasticNetParam`), which control aspects of the model training process like the number of iterations, regularization parameter, and the mix of L1 and L2 regularization.\n",
    "\n",
    "5. **Train the Model**:\n",
    "   - The model is trained (fitted) on the provided DataFrame `df`. The `fit` method applies the linear regression algorithm to learn the relationship between the features and the label.\n",
    "\n",
    "6. **Output Model Parameters**:\n",
    "   - After training, the model's coefficients (weights assigned to the features) and the intercept (the point where the estimated regression line crosses the y-axis) are printed out. These parameters define the fitted line in linear regression.\n",
    "\n",
    "In summary, this code is a basic example of performing linear regression in PySpark, where it trains a model to understand the relationship between a single feature and a label, then outputs the parameters of the learned linear relationship.\n",
    "\n",
    "---\n",
    "\n",
    "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. Here are the key points:\n",
    "\n",
    "1. **Basic Idea**: The main goal of linear regression is to find a linear relationship between the independent variable(s) (also known as predictors or features) and the dependent variable (also known as the response or outcome). \n",
    "\n",
    "2. **Linear Equation**: This relationship is represented as a linear equation, typically in the form `y = mx + c` for simple linear regression with one independent variable, where:\n",
    "   - `y` is the dependent variable.\n",
    "   - `x` is the independent variable.\n",
    "   - `m` is the slope of the line (shows how much `y` changes for a unit change in `x`).\n",
    "   - `c` is the y-intercept (value of `y` when `x` is 0).\n",
    "\n",
    "3. **Multiple Variables**: In cases with more than one independent variable, the equation becomes `y = b0 + b1*x1 + b2*x2 + ... + bn*xn`, where `b0` is the intercept and `b1`, `b2`, ..., `bn` are coefficients for each independent variable `x1`, `x2`, ..., `xn`.\n",
    "\n",
    "4. **Fitting the Model**: \"Fitting\" a linear regression model involves finding the values of the coefficients that result in the best approximation of the actual relationship between the variables. This is usually done by minimizing the difference between the observed values and the values predicted by the model (often using a method called least squares).\n",
    "\n",
    "5. **Use Cases**: Linear regression is used in various fields like economics, biology, engineering, etc., for predicting a quantitative response, understanding relationships between variables, and for trend forecasting.\n",
    "\n",
    "In simple terms, linear regression is like drawing a straight line through data points in a way that the line represents the best estimate of how those points relate to each other.\n",
    "\n",
    "---\n",
    "\n",
    "Note: for the Code below to work, we need to install `numpy` => `pip3 install numpy`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/11 22:33:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.657728271247261]\n",
      "Intercept: 1.3422717287527388\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Sample data\n",
    "data = [(Vectors.dense([0.0]), 1.0),\n",
    "        (Vectors.dense([1.0]), 2.0),\n",
    "        (Vectors.dense([2.0]), 3.0)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\", \"label\"])\n",
    "\n",
    "# Linear Regression model\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(df)\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
